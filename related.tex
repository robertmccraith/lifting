\section{Related work}\label{s:related}

%sdf skips annotation when Mask-rcnn box less than 50% of GT 
%https://github.com/TRI-ML/sdflabel/blob/master/pipelines/refine_css.py line 110
%line 186 seems t change estimate of height based on how good the iou of the model is vs rcnn/gt box

% \paragraph{Object Detection in 2D} %??.?

\paragraph{Supervised.}

3D object detection methods assume availability of either monocular RGB images, LiDAR point clouds or both. Here we focus on supervised methods using 3D point cloud inputs. \cite{wang2015voting,engelcke2017vote3deep} discretise point clouds onto a sparse 3D feature grid and apply convolutions while excluding empty cells. \cite{chen2017multiview} project the point cloud onto the frontal and the top views, apply 2D convolutions thereafter and generate 3D proposals with an RPN \cite{ren16faster}. \cite{zhou2018voxelnet} convert the input point cloud into a voxel feature grid, apply a PointNet \cite{qi16pointnet:} to each cell and subsequently process it with a 3D fully convolutional network with an RPN head which generates object detections.
Frustum PointNets \cite{qi2017frustum} is a two step detector which first detects 2D bounding boxes using these to determine LiDAR points of interest which are filtered further by a segmentation network. The remaining points are then used to infer the 3D box parameters with the centre prediction being simplified by some intermediate transformations in point cloud origins. We are using Frustum PointNets as a backbone for our method.

\paragraph{Weakly Supervised.}

Owing to the complexity of acquiring a large scale annotated dataset for 3D object detection many works recently have attempted to solve this problem with less supervision. \cite{meng2020ws3d} the required supervision is reduced from the typical $(X,Y,Z)$ centre, $yaw$, $(x_1,y_1,x_2,y_2)$ 2D box and $(l,w,h)$ 3D size they instead annotate 500 frames with centre $(X,Z)$ in the Birds Eye View and finely annotating a 534 car subset of these frames to achieve accuracy similar to models trained on the entire Kitti training set. This however can result in examples in the larger training set or validation set which are outside the distribution of cars seen in the smaller subset, are also susceptible to the problems mentioned in \cite{feng2020labels} and the weakly annotated centres can have a large difference to the Kitti annotations. In \cite{sdflabel} trains DeepSDF\cite{Park_2019_CVPR} on models from a synthetic dataset which are then rendered into the image and iteratively refined to produce a prediction that best fits the predictions of Mask R-CNN outputs.
% {\color{red} (which are sufficiently close to ground truth 2D boxes)}. 
This however takes 6 seconds to refine predictions on each input example and ground truth 2D boxes are used to select cars used to train on. In \cite{qin20weakly} 3D anchors densely placed across the range of annotations are projected into the image with object proposed by looking at the density of points within or nearby the anchors in 3D and the 3D pose and 2D detections are supervised by a CNN trained on Beyond PASCAL\cite{xiang14beyond}. In \cite{koestler2020learning} instance segmentation is used to place a mesh in the location of detected cars which is refined using supervised depth estimation. 

\paragraph{Viewpoint estimation.}
Estimating 3D camera orientation is an actively researched topic \cite{pepik12teaching, kendall2015posenet, tulsiani15viewpoints, su2015render, mousavian20173d, prokudin2018deep, liao2019spherical}. Central to this problem is the choice of an appropriate representation for rotations. Directly representing rotations as angles $\theta \in [0, 2\pi )$ suffers from discontinuity at $2\pi$. One way to mitigate this is to use the trigonometric representation $(\cos \theta, \sin \theta)$ \cite{penedones2012improving, massa2016crafting, beyer2015biternion}. Another approach is to discretise the angle into $n$ quantised bins and treat viewpoint prediction as a classification problem \cite{tulsiani15viewpoints, su2015render, mousavian20173d}. Quaternions are another popular representation of rotations used with neural networks \cite{kendall2015posenet, xiang2017posecnn}. Learning camera pose without direct supervision, for example, when fitting a 3D model to 2D observations, may suffer from ambiguities due to the object symmetries \cite{saxena2009learning}. Practically, this means that the network can get stuck in a local optima of $SO(3)$ space and not be able to recover the correct orientation. Several recent works \cite{tulsiani2018multi, insafutdinov18pointclouds, goel20shape} overcome this by maintaining several diverse candidates for the estimated camera rotation and selecting the one that yields the best reconstruction loss. Our approach discussed in sec.~\ref{s:direct-yaw} is most related to these methods.

%Predicting Yaw as one of $n$ quantised steps is well studied approach in supervised learning approaches. Compared to continuous predicted values in the vector and quaternion approaches this does not have an issue with the discontinuity present at the jump between $0$ and $2\pi$ where the the loss value is quite high but the actual error is relatively quite small.} 