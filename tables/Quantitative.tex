\begin{table*}[t]
\newcommand{\zero}{ZERO}
\centering

\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{1.8mm}{
\begin{tabular}{c | c | c c | c | c c c }
 \toprule
 \multirow{2}{*}{Paradigm} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{External Data Supervision} & & \multicolumn{3}{c}{AP\textsubscript{BEV}~/~AP\textsubscript{3D} (IoU = 0.5)} \\
 & & COCO 2D Detection & Pascal3d & Kitti GT 2D Boxes & Easy & Moderate & Hard \\\hline


% \multirow{3}{*}{\textit{\makecell{ Weakly \\ supervised}}}
%   &PCL~\cite{tang2018pcl}      &  & & & 1.878 /- &1.058 /- &0.935 /- \\
%   &OICR~\cite{tang2017oicr}    &  & & & 6.481 /- &2.933 /- &3.270 /- \\
%   &MELM~\cite{wan2018min}      &  & & & 2.796 /- &1.486 /- &1.476 /- \\
%     \hline

\multirow{1}{*}{\textit{\makecell{Supervised}}}
  &Frustum PointNet~\cite{qi2017frustum}      & & & & 98.25/98.10 & 94.92/94.31 & 87.14/86.48 \\
    \hline

%   &VS3D\cite{meng2020ws3d} & Mono   & 2d box,Yaw & 76.93 / 31.35 & 71.84 / 23.92 & 59.39 / 19.34 \\
%   &VS3D\cite{meng2020ws3d} & Stereo & 2d box,Yaw & 79.03 / 40.98 & 72.71 / 34.09 & 59.77 / 27.65 \\

  
   
   
%   &VS3D\cite{meng2020ws3d} & Mono + LiDAR    & Yaw from Image & 81.60 / 41.83 & 72.43 / 39.22 & 64.31 / 32.73 \\
%   &VS3D\cite{meng\2020ws3d} & Stereo + LiDAR  & Yaw from Image & 81.95 / 42.43 & 73.21 / 41.58 & 64.34 / 32.74 \\
%   \hline
  &VS3D\cite{meng2020ws3d} &  & \cellcolor{green} 2D Detection, Yaw & & 74.54 / 40.32 & 66.71 / 37.36 & 57.55 / 31.09 \\ 
  
  
\multirow{3}{*}{\textit{\makecell{Unsupervised}}}
   &Zakharov et al. \cite{sdflabel} & \cellcolor{green}\checkmark & & \cellcolor{green}\checkmark & 77.84/62.25 & 59.75/42.23 & -/- \\
   % Cremers paper here?
   
   &Ours &\cellcolor{green}\checkmark & &  & \textbf{82.37}/\textbf{78.48} & \textbf{82.57}/\textbf{77.63} & \textbf{74.40}/\textbf{69.90} \\
   
\end{tabular}}} 
\label{t:averageprecision}

\caption{Object Detection Average-Precision on the Kitti validation set. Compared to our Method VS3D\cite{meng2020ws3d} uses a network trained on Pascal 3D and NYC 3D Cars\cite{xiang_wacv14, MatzenICCV13} to determine the object Yaw and 2D box, while while Zakharov\cite{sdflabel} only considers MASK R-CNN detections with an IOU > $50\%$ compared to a ground truth box and uses a synthetic dataset to train a network which gives yaw.}

\end{table*}